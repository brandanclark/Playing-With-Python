#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jul  6 11:22:42 2022
Using Python to complete a logistic regression algorithm
From: https://towardsdatascience.com/a-complete-logistic-regression-algorithm-from-scratch-in-python-step-by-step-ce33eae7d703
@author: brandanclark
"""

#importing packages and data
import pandas as pd
import numpy as np
df = pd.read_csv('/Users/brandanclark/Desktop/GitHubFolder/heart.csv')
df.head()

#define a hypothesis

def hypothesis(X, theta):
    z = np.dot(theta, X.T)
    return 1/(1+np.exp(-(z))) - 0.0000001

#define the cost function

def cost(X, y, theta):
    y1 = hypothesis(X, theta)
    return -(1/len(X)) * np.sum(y*np.log(y1) + (1-y)*np.log(1-y1))

#update theta values

def gradient_descent(X, y, theta, alpha, epochs):
    m =len(X)
    J = [cost(X, y, theta)]
    for i in range(0, epochs):
        h = hypothesis(X, theta)
        for i in range(0, len(X.columns)):
            theta[i] -= (alpha/m) * np.sum((h-y)*X.iloc[:, i])
        J.append(cost(X, y, theta))
    return J, theta

#calculate the final prediction and accuracy

def predict(X, y, theta, alpha, epochs):
    J, th = gradient_descent(X, y, theta, alpha, epochs)
    h = hypothesis(X, theta)
    for i in range(len(h)):
        h[i]=1 if h[i] >=0.5 else 0
    y = list(y)
    acc = np.sum([y[i] == h[i] for i in range(len(y))])/len(y)
    return J, acc 

#view data

df.head(10)

#add extra column to dataset for bias

df = pd.concat([pd.Series(1, index = df.index, name = '00'), df], axis=1)

#define input features and output variables

X = df.drop(columns=["00", "cp", "thal"])
y = df["target"]

#get the accuracy result

theta = [0.5]*len(X.columns)
J, acc = predict(X, y, theta, 0.0001, 25000)

#accuracy result around 80%, per output of above function

#attempt at mapping this into a plot

%matplotlib inline
import matplotlib.pyplot as plt
plt.figure(figsize = (12, 8))
plt.scatter(range(0, len(J)), J)
plt.show()

#cost descends very rapidly in the beginning, suggesting a perfect cost function